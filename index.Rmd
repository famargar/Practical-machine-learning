---
title: "PML_FinalProject"
author: "Fabrizio"
date: "July 10, 2016"
output: html_document
---

# Prediction project for the Practical Machine Learning course

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. The goal of your project is to predict the manner in which they did the exercise. 

The "classe" variable contains 5 levels: A,B,C,D,E. You may use any of the other variables to predict with. You should create a report describing 
1) how you built your model
2) how you used cross validation
3) what you think the expected out of sample error is
4) and why you made the choices you did. 
5) you will also use your prediction model to predict 20 different test cases.

## Load packages

```{r}
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(corrplot))
```

Now reading the training and testing data, cleaning both datasets from un-needed variables: the project requires to read only data from belt/arm/dumbell sensors. Also removing columns with empty values, and Not Assigned (NA) values.

```{r}
training <- read.csv("pml-training.csv")
testing  <- read.csv("pml-testing.csv")
filter = grepl("belt|arm|dumbell|classe", names(training))
training = training[, filter]
testing = testing[, filter]
cols.without.na = colSums(is.na(testing)) == 0
training = training[, cols.without.na]
testing = testing[, cols.without.na]
```

## Preprocessing

Trasnform "classe" in a numeric column. Check zero variance predictors. 

```{r}
num.class = length(levels(training$classe))
levels(training$classe) = 1:num.class
head(training)
zero.var = nearZeroVar(training, saveMetrics=TRUE)
zero.var
```

Now create a cross-validation sample that will be used to estimate the out-of-sample error

```{r}
inTrain <- createDataPartition(training$classe, p = 0.6, list = FALSE)
training <- training[inTrain, ]
validate <- training[-inTrain,]
```

## A Boosted Decision Tree model

Now create a model training it on the 60% of the dataset provided in the project, using boosted decision trees.
Checking also the importance of the predictors.

```{r}
boostFit <- train(classe ~ ., method = "gbm", data = training, verbose = F, trControl = trainControl(method = "cv", number = 10))
boostFit
plot(boostFit, ylim = c(0.9, 1))
imp <- varImp(boostFit)$importance
imp$max <- apply(imp, 1, max)
imp <- imp[order(imp$max, decreasing = T), ]
imp
```

## Out of sample model error

Now estimate the out of sample error

```{r}
# predict on testValidateData
predictions_val <- predict(boostFit, validate)
# true accuracy of the predicted model
outOfSampleError.accuracy <- sum(predictions_val == validate$classe)/length(predictions_val)
# out of sample error and percentage of out of sample error
outOfSampleError <- 1 - outOfSampleError.accuracy
outOfSampleError
e <- outOfSampleError * 100
```

the out-of-sample error computed on the cross-validation sample is `r round(e, digits = 2)`%.

## The prediction

We can now compute the predictions to be inpute as the final outcome of this project

```{r}
prediction <- as.character(predict(boostFit, testing))
prediction
```
